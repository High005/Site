[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Useful tutorial for your data journey",
    "section": "",
    "text": "Build your first data collection tool :\n1 - How to collect quantitative and discrete data in REDCap:\n\n\n\n\n\n I RedCap Tutorial \n\n\n2 - How to build calculated field & automated tools:\n\n\n\n\n\n II RedCap Tutorial \n\n\n3 - Weighted formula on calculated field:\n\n\n\n\n\n IIb RedCap Tutorial \n\n\n4 - How to export data your data from REDCap to Excel:\n\n\n\n\n\n III RedCap Tutorial \n\n\nemail : bisturiep005@gmail.com"
  },
  {
    "objectID": "science.html",
    "href": "science.html",
    "title": "Statistics tips",
    "section": "",
    "text": "Easies way to show your statistical output to everybody\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Deviation Vs Standard Error\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nYour P-value is much more than you think\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBias in Healthcare studies\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival analysis\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAspirin Vs Cancer\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/datah/papers/bias/index.html",
    "href": "posts/datah/papers/bias/index.html",
    "title": "Bias in Healthcare studies",
    "section": "",
    "text": "The Bias in healthcare studies\nWhen practicing evidence-based medicine, medical doctors and healthcare researchers often on data analysis to make informed decisions. However, one of the biggest challenges in research is bias—systematic errors that can distort findings and lead to incorrect conclusions.\nSince then many choices in medical practice are made from fresh research papers it’s better to be well trained on get the idea of bias.\nHere the main types of bias healthcare are explained with some examples, and offers strategies to reduce them. Bias in research can occur at various stages from study design to data collection and publication.\nSo, let’s find the bias when working in data analytics for evidence-Based Medicine.\n\n\nType of Bias:\n\nSelection Bias\n\nSelection bias occurs when the participants included in a study are not representative of the target population. This often happens when the selection process is influenced by factors that are related to the outcome of interest.\nExample: A clinical trial on a new hypertension drug recruits only younger, healthier patients, excluding older adults with comorbidities. The results may show the drug is effective, but this might not hold true for the broader population.\nWhy It Matters: Selection bias can limit the generalizability of study findings, making it hard to apply results to real-world clinical practice.\n\nObservation Bias\n\nObservation bias arises when there are errors in measuring exposure, outcomes, or other variables due to the way data is collected.\nExample: In a study comparing pain levels after two types of surgeries, patients in one group know they received the “innovative” surgery and report lower pain levels because they expect better outcomes. This is an example of performance bias, a subtype of observation bias.\nWhy It Matters: Observation bias can artificially inflate or deflate the apparent effectiveness of an intervention.\n\nPublication Bias\n\nPublication bias occurs when studies with significant or “positive” results are more likely to be published than studies with “negative” or null results.\nExample: A pharmaceutical company sponsors ten trials for a new antidepressant. Only the three trials showing significant improvements are published, while the seven with no benefit are ignored. This creates a skewed perception of the drug’s effectiveness.\nWhy It Matters: Clinicians and researchers rely on published literature. If only favorable studies are accessible, it may lead to overestimation of treatment benefits.\n\nConfirmation Bias\n\nConfirmation bias occurs when researchers unconsciously interpret or highlight data in ways that support their hypothesis.\nExample: A researcher studying the benefits of a specific diet for diabetes patients may focus on data points that show improvements while ignoring data showing no change or worsening outcomes.\nWhy It Matters: Confirmation bias can undermine the objectivity of scientific inquiry and mislead clinical decision-making.\n\nAttrition Bias\n\nAttrition bias happens when participants drop out of a study in a way that is related to the exposure or outcome.\nExample: In a weight loss study, participants who struggle to lose weight are more likely to drop out. The final results may exaggerate the effectiveness of the program because only those who succeeded remain.\nWhy It Matters: Attrition bias can distort findings, particularly in long-term studies.\n\n\nHow to Reduce Bias?\nReducing bias is critical to ensure the validity and reliability of research findings. Here are some strategies:\n\nRandomization\n\nRandomization ensures that participants are assigned to groups purely by chance. This minimizes selection bias and balances known and unknown confounding factors between groups.\nExample: In a randomized controlled trial for a new vaccine, participants are randomly assigned to receive either the vaccine or a placebo.\n\nBlinding\n\nBlinding prevents participants, researchers, or both from knowing which intervention has been assigned, reducing observation bias.\n\nSingle-blind: The participants are unaware of their group allocation.\nDouble-blind: Both participants and researchers are unaware of group allocation.\n\nExample: In a drug trial, neither the patients nor the clinicians administering the medication know who receives the active drug versus the placebo.\n\nPre-Registration of Study Protocols\n\nResearchers can pre-register their study design, hypotheses, and planned analyses in databases like ClinicalTrials.gov. This reduces the risk of selective reporting and confirmation bias.\nExample: By pre-registering, a researcher commits to publishing the findings, whether the results are significant or not.\n\nIntention-to-Treat Analysis\n\nThis approach analyzes participants based on the group to which they were originally assigned, regardless of whether they completed the intervention as planned. It helps address attrition bias.\nExample: In a diabetes drug trial, a patient who stops taking the drug halfway through is still included in the final analysis.\n\nSystematic Reviews and Meta-Analyses\n\nSystematic reviews and meta-analyses combine results from multiple studies, helping to counteract publication bias by including both published and unpublished data.\nExample: A systematic review of antidepressant trials includes data from pharmaceutical companies that were not published in journals.\n\nTransparent Reporting\n\nAdhering to reporting guidelines, such as CONSORT for clinical trials or PRISMA for systematic reviews, ensures all relevant details are disclosed.\nExample: A clinical trial includes detailed information about randomization, blinding, and attrition rates in its published report.\n\nIndependent Replication\n\nEncouraging independent replication of studies helps verify findings and reduce the influence of biases in individual studies.\nExample: If multiple independent studies show the same results for a new cancer treatment, confidence in its effectiveness increases.\nWhat’s to do now?\nBias is a common but manageable challenge in evidence-based medicine. By understanding the different types of bias such as: selection, observation, publication, confirmation, and attrition biases and employing strategies like: randomization, blinding, and transparent reporting. Healthcare researcher can now critically appraise research and make better clinical decisions. Reducing bias not only strengthens the validity of studies but also ensures that patients receive the best possible care based on reliable evidence.\nMaterial to go even deeper on the topic:\n\nHiggins JPT, Green S. Cochrane Handbook for Systematic Reviews of Interventions. Version 5.1.0. The Cochrane Collaboration, 2011.\nMoher D, Liberati A, Tetzlaff J, Altman DG. “Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement.” PLoS Med. 2009;6(7):e1000097.\nSchulz KF, Altman DG, Moher D. “CONSORT 2010 Statement: updated guidelines for reporting parallel group randomised trials.” BMJ. 2010;340:c332.\nIoannidis JPA. “Why most published research findings are false.” PLoS Med. 2005;2(8):e124."
  },
  {
    "objectID": "posts/datah/medical_datactor/tech_challenge/index.html",
    "href": "posts/datah/medical_datactor/tech_challenge/index.html",
    "title": "Is tech improving our healthcare journey?",
    "section": "",
    "text": "Is tech improving our healthcare journey?\nThe integration of digital technologies in healthcare is an evolutionary process, marked by both advancements and initial challenges. While the introduction of new technologies can lead to temporary setbacks and integration hurdles, the healthcare sector is currently experiencing a transformative period where data and technologies like artificial intelligence (AI) are converging to enhance processes, capabilities, and, most importantly, patient care. The following outlines some significant shifts occurring in healthcare delivery and medication management within complex supply chains and hospital systems.\nFrom Basic Automation to Intelligent and Adaptive Assistance:\nThe 21st century has witnessed a surge in the adoption of automation in healthcare. However, early automation systems presented their own set of difficulties. While hospital pharmacies have realized certain efficiencies, these technologies have also introduced challenges, such as information overload.\nMany communication systems, designed with good intentions, may have been implemented without a comprehensive understanding of hospital workflows. This has resulted in excessive and often unnecessary communication, contributing to staff burnout, particularly in pharmacy settings where personnel are bombarded with irrelevant or low-value notifications. Consequently, there’s a critical need for more user-centric and adaptive systems.\nAI is paving the way for such systems, enabling greater customization based on individual user needs. Instead of being designed solely around a tool’s capabilities, features and notifications can be tailored to specific user requirements. This represents a significant advancement in hospital-based digital transformation, shifting the focus from simply having information available to optimizing how that information is accessed and utilized.\nThe importance of this customization becomes clear when considering the complex tasks healthcare professionals perform. For instance, an anesthesiologist in the operating room might be managing numerous concurrent tasks, including monitoring patient vitals, interacting with electronic health records (EHRs), and tracking medication administration. Currently, this information often originates from disparate sources. The goal is to consolidate and deliver this information to providers in a timely and relevant manner. The future envisions technology as an extension of the provider, potentially through innovations like virtual and augmented reality, providing real-time access to crucial information.\nEnhancing Supply Chain Management through Predictive Analytics and Collaboration:\nEffective pharmaceutical forecasting and management are essential for both operational efficiency and patient well-being. However, maintaining a balance between overstocking and stockouts can be challenging.\nDrug shortages remain a persistent problem in healthcare. Conversely, excessive stockpiling can lead to waste through expiration or unnecessary inventory costs. Machine learning (ML) offers a promising solution for supply chain optimization. Centralizing data by integrating technologies across health systems creates a unified data repository. This repository provides real-time visibility into inventory, usage trends, expirations, recalls, and other critical data points.\nBy leveraging this comprehensive data, ML algorithms can generate analytical insights and predictive models to anticipate future needs and optimize ordering processes. This allows for more efficient distribution of pharmaceuticals within hospital systems, freeing pharmacists to focus on patient care.\nFurthermore, cloud-based technologies facilitate inter-hospital collaboration. Moving away from siloed data centers enables greater data sharing and transparency. This is particularly beneficial for managing pharmaceutical supply chains, which can be unpredictable. Shared visibility can mitigate the risk of hoarding and ensure that critical therapies are available where they are needed most, especially during shortages or emergencies.\nPrioritizing Ethical and Secure Transformation:\nAs hospitals embrace digital transformation, cybersecurity must be a paramount concern. Protecting sensitive patient information and ensuring the reliability and integrity of technology are crucial. Given the increasing sophistication of cyber threats, including AI-driven attacks, hospitals must be vigilant in vetting software and vendors, both new and existing. Legacy systems, often overlooked, also require thorough scrutiny.\nBeyond technology assessment, establishing robust cybersecurity governance, risk, and compliance (GRC) programs and hiring qualified personnel are essential. All vendors should adhere to strict compliance standards.\nWhen implemented thoughtfully and securely, digital transformation can enhance operational efficiency, reduce staff burden, and improve patient care. Ultimately, the greatest success will be measured by the positive impact on the patient experience."
  },
  {
    "objectID": "posts/datah/medical_datactor/longevity/index.html",
    "href": "posts/datah/medical_datactor/longevity/index.html",
    "title": "ChatGPT and his Artificial intelligence will make us live longer",
    "section": "",
    "text": "ChatGPT and his Artificial intelligence will make us live longer\nOpenAI has recently announced an innovative partnership with Retro Biosciences, a biotech startup dedicated to advancing longevity research. This collaboration has resulted in the creation of GPT-4b Micro, a state-of-the-art AI model designed to expedite biological research focused on extending human lifespans by a decade or more. This initiative reflects a growing trend where artificial intelligence plays a pivotal role in biological and medical sciences, with OpenAI’s latest innovation poised to significantly impact longevity science.\nIntroducing GPT-4b Micro GPT-4b Micro is a specialized variant of OpenAI’s renowned GPT-4 model, tailored specifically for biological data analysis. Unlike the general-purpose GPT-4, this version has undergone extensive training on protein sequence datasets, enabling it to decipher protein interactions and suggest ways to manipulate them for research purposes.\nThe AI focuses primarily on a group of proteins known as the Yamanaka factors. These proteins have the remarkable ability to reprogram mature human skin cells into a more youthful, stem cell-like state. Scientists believe that by fine-tuning the behavior of these proteins, it may be possible to rejuvenate human cells, paving the way for longer lifespans and healthier aging.\nA Novel Approach to Protein Engineering\nGPT-4b Micro distinguishes itself from other AI models like Google’s AlphaFold through its unique approach to protein engineering. While AlphaFold excels at predicting protein structures, GPT-4b Micro takes a step further by generating novel protein variants for experimental testing. The model uses “few-shot” prompts—akin to how ChatGPT crafts text suggestions—to propose innovative combinations of proteins that might yield significant biological advancements.\nBy analyzing its extensive training data, GPT-4b Micro generates protein designs that researchers can validate experimentally. This methodology offers a transformative perspective, presenting scientists with creative experimental directions that may otherwise have gone unexplored. Essentially, GPT-4b Micro serves as a virtual architect for protein innovation.\nPromising Early Results\nInitial experiments with GPT-4b Micro have shown encouraging outcomes. Protein variants suggested by the AI have outperformed those designed by human researchers, with some achieving performance improvements of up to 50-fold for specific proteins. While these findings are promising, they require further validation through rigorous testing and peer-reviewed studies.\nShould these results withstand scrutiny, they could represent a major leap forward in longevity science. Enhanced proteins capable of rejuvenating cells may significantly extend human life and healthspan, aligning with Retro Biosciences’ mission to leverage AI in the fight against aging.\nWhy It Matters\nThis development highlights the transformative potential of AI in accelerating discoveries in biology and medicine. OpenAI’s CEO, Sam Altman, has expressed optimism about AI’s ability to drive scientific progress, and GPT-4b Micro exemplifies this vision by contributing to advancements in longevity research.\nNotably, Sam Altman has personally invested $180 million in Retro Biosciences, underscoring the growing interest and importance of this field. The collaboration between AI and biotechnology represents an exciting frontier in addressing humanity’s grand challenges, with profound implications for aging populations and global health.\nLooking Ahead\nDespite the promising early results, the journey to fully validate and refine GPT-4b Micro’s protein designs remains complex. However, the partnership between OpenAI and Retro Biosciences sets a precedent for integrating AI with biological research to tackle age-related challenges. By combining computational power with scientific inquiry, they are advancing efforts to extend human lifespans and improve the quality of life on an unprecedented scale.\nAs AI continues to revolutionize biological sciences, it is clear that these technologies will play an increasingly critical role in addressing some of humanity’s most pressing issues. While the ultimate impact of GPT-4b Micro on longevity research is yet to be determined, the collaboration between OpenAI and Retro Biosciences marks an important step toward a future where AI-driven solutions reshape our understanding of aging and health."
  },
  {
    "objectID": "posts/datah/medical_datactor/bitcoin/index.html",
    "href": "posts/datah/medical_datactor/bitcoin/index.html",
    "title": "Bitcoin like solutions and decentralized payments can help your healthcare’s balance",
    "section": "",
    "text": "The Bitcoin era\nDifferent medical practices experience varying degrees of benefit from cryptocurrency adoption. Practices that primarily operate on a cash basis, such as outpatient clinics and those offering concierge or direct primary care (where patients pay a recurring fee for enhanced access), can enhance their modern image by accepting cryptocurrency. The growing field of medical tourism also stands to gain. For patients traveling from countries with unstable local currencies or unfavorable exchange rates, cryptocurrency offers a potentially stable and convenient alternative. For example, in El Salvador, Bitcoin is legal tender, and in Argentina, where inflation is high and dollar exchange rates fluctuate significantly, cryptocurrency can provide a way to navigate financial uncertainty.\nWhile credit cards are popular for their convenience, their transaction fees (typically 3-5%) can significantly impact a practice’s revenue. Cryptocurrency payment gateways, however, often have much lower fees, sometimes as low as 0.5%. In the context of rising inflation and shrinking profit margins in healthcare, this difference can be crucial for maintaining profitability.\nThe potential of smart contracts, self-executing agreements coded on the blockchain, is a key aspect of this technology. These contracts automate transactions and reduce the need for intermediaries. In healthcare, they could streamline billing by releasing payments automatically upon service completion, such as after a telemedicine appointment, saving time and minimizing disputes.\nSmart contracts can also improve regulatory compliance by embedding legal requirements directly into transactions, enhancing security and trust. While implementation requires technical expertise, the resulting transparency and efficiency can be valuable. For practices using cryptocurrency, smart contracts offer additional innovation and functionality.\nDirect cryptocurrency transfers between patient and practice wallets, while technically possible and fee-free, are prone to errors and generally not recommended. Instead, practices should use cryptocurrency payment gateways. These platforms, similar to credit card processors, bridge the gap between patient wallets and the practice’s bank account, adding security layers. These gateways often mitigate price volatility by locking in exchange rates for a short period during transactions.\nUpon receiving payment, the practice can choose to retain the cryptocurrency or convert it to traditional currency. Various gateways exist, each with different fees and supported cryptocurrencies. Some offer reduced or waived fees for premium memberships, which may be beneficial for practices with high cryptocurrency transaction volumes.\nTo begin accepting cryptocurrency, a practice can:\nCreate an account on a reputable and regulated cryptocurrency exchange (e.g., Coinbase, Binance). Integrate the chosen platform with their website or provide in-office QR codes for payments. Decide whether to hold cryptocurrency or convert it to traditional currency. Train staff to manage transactions and monitor payments using the platform’s interface. Many gateways, including Coinbase and PayPal, offer user-friendly app-based payment options.\nWhen choosing which cryptocurrencies to accept, it’s important to be aware of the risks, including the prevalence of fraudulent or unstable currencies. Starting with Bitcoin (BTC), the most widely accepted cryptocurrency with the largest market capitalization, is advisable. Another option is USD Coin (USDT), a stablecoin pegged to the US dollar, which simplifies conversion to traditional currency.\nEven major cryptocurrencies can fluctuate significantly in value. This can lead to patient dissatisfaction if the value changes substantially after a transaction. Therefore, obtaining informed consent from patients before accepting cryptocurrency payments and clearly explaining the potential for price volatility is crucial. It’s important to acknowledge that price fluctuations could benefit or disadvantage the patient.\nWhile cryptocurrency offers a promising payment option for healthcare, it’s unlikely to completely replace traditional methods. Practices should maintain their existing payment infrastructure. Further research is needed to determine the prevalence of cryptocurrency use in healthcare transactions.\nCryptocurrency represents a potential evolution in healthcare finance. By adopting this technology, practices can modernize, potentially improve patient satisfaction, and streamline operations. Whether a practice is cash-based and seeking cost reductions or a medical tourism provider aiming to expand its reach, cryptocurrency offers potential benefits. A measured approach, continuous learning, and adaptability are key as this technology continues to develop."
  },
  {
    "objectID": "jobs.html",
    "href": "jobs.html",
    "title": "Let’s keep in touch to find solutions",
    "section": "",
    "text": "Affiliations and ongoing projects:\n\nWith you [today - future]\nData Scientist and programmer for Diecimilauno S.P.A. (Treviso, IT) [July 2024 - ongoing]\nStealth deeptech startup (Verona, IT) [2024 - ongoing]\nASST Ospedale Papa Giovanni XXIII di Bergamo [2024 - ongoing]\nMainz University (Mainz, DE) [2024 - Accomplished]\nPadua University with the department of: Cardiology - internal Medicine - Surgery - Oncology (Padua, IT) [2021 - ongoing]\n\nemail : bisturiep005@gmail.com"
  },
  {
    "objectID": "Chicche.html",
    "href": "Chicche.html",
    "title": "Chicche",
    "section": "",
    "text": "Here, we will walk through an example of performing GSEA in R…\nlibrary(clusterProfiler) # Example code for GSEA"
  },
  {
    "objectID": "Chicche.html#easy-gene-set-enrichment-analysis-in-r",
    "href": "Chicche.html#easy-gene-set-enrichment-analysis-in-r",
    "title": "Chicche",
    "section": "",
    "text": "Here, we will walk through an example of performing GSEA in R…\nlibrary(clusterProfiler) # Example code for GSEA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About High 0.05",
    "section": "",
    "text": "It all started with debates with University Professors where bottles were party where delivered when the P-value of the analysis in progress showed was less than 0.05.\nAt that moment smiles blossomed and above all we began to share, as the English say, the “High five” with researchers of every background and specialty from biologists, research nurses to Medical Doctors. To dedicate even an additional dose to that magical number, the whole company declared his love by adopting the name of so loved P = 0.05. The need to further explore a multidisciplinary approach is since technology is just a tool that can became useful if it brings large-scale changes to everyday life and can identify concrete and high-impact fields of application."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "High 005",
    "section": "",
    "text": "Click to go deeper on your curiosity\n\n\n\n\n\n\n\n\n\n\n\n\nNewscare\n\n\nThe news about revolution that are solving Medicine’s challenges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe DeepSeek deception: How fake accounts fooled markets and what it means for AI investment\n\n\n\n\n\n\n\n\nSep 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimplest Way to Present Your Statistical Results\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Deviation Vs Standard Error\n\n\n\n\n\n\n\n\nSep 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour P-value is much more than you think\n\n\n\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Artificial Intelligence is improving the teaching journey in medical oncology\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/datah/index.html",
    "href": "posts/datah/index.html",
    "title": "Newscare",
    "section": "",
    "text": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow Artificial Intelligence is improving the teaching journey in medical oncology\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow Novo Nordisk Accelerates drug development times from 12 weeks to 10 minutes with GenAI\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe world’s smallest surgical robot is among us\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT and his Artificial intelligence will make us live longer\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIs tech improving our healthcare journey?\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBitcoin like solutions and decentralized payments can help your healthcare’s balance\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nArtificial intelligence will improve your healthcare job routine\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/datah/medical_datactor/cardiology/index.html",
    "href": "posts/datah/medical_datactor/cardiology/index.html",
    "title": "Artificial intelligence will improve your healthcare job routine",
    "section": "",
    "text": "AI and Medical journey\nLLMs in medicine are nowadays super in hype. Or better, GenAI is super in hype. The last one is different from traditional AI in that it can generate content, such as summaries of text and answers to questions, rather than merely analyzing existing data. GenAI can definitively improve your work as healthcare professional since the goal is just to “removing the robot that is inside you to let you perform more human related task”. Hence the opportunity here of GenAI is just to manage exactly all the jobs that as healthcare professional you try to avoid every day to let you as much time as possible with patients. This since GenAI can empower your role, plus the patients to take a more active role in their healthcare. This since patients can use LLM-powered chatbots to interact with their EHR, prepare for appointments.\nWhile challenges exist, the benefits of GenAI are too significant to ignore, and it may revolutionize care delivery by automating tasks, improving patient self-management, and enhancing job satisfaction.\nBy actively engaging in the development and implementation of GenAI, and adopting a balanced approach that prioritizes safety and responsibility, we can explore its potential to transform the way we deliver cardiovascular care and ultimately improve patient outcomes."
  },
  {
    "objectID": "posts/datah/medical_datactor/nano_robots/index.html",
    "href": "posts/datah/medical_datactor/nano_robots/index.html",
    "title": "The world’s smallest surgical robot is among us",
    "section": "",
    "text": "The multifunctional biomedical robot can do imaging, sampling, and laser ablation.\nEngineers at the Hong Kong University of Science and Technology have developed a new surgical robot, maybe the world’s smallest one. The 0.95-millimeter robot is 60% smaller than current endoscopic robots and is made of a hollow skeleton, optical fibers, functionalized skin, and a gel-like coating.The researchers say it achieves the “impossible trinity” of combining imaging, precise movement, and multiple functions. Their findings were published in Nature Communications.\nThe study showed the robot significantly expands the imaging area (25 times the normal view) and can detect obstacles up to 9.4 mm away (10 times the theoretical limit). It’s capable of sampling, drug delivery, and laser ablation, and navigated smoothly through in vitro bronchial models and ex-vivo pig lungs.\nProfessor Shen Yajing, who led the research team, explained that while small robots have potential for diagnosis and treatment, current models lack compactness, precise navigation, and diverse functions. Their work aims to solve these problems. Prof. Shen believes this robot could be a major step forward in clinical surgical robots, enabling early diagnosis and treatment in difficult-to-reach areas of the body, with broad biomedical applications. The team plans to conduct in vivo trials next.\nThe Asia-Pacific region is seeing increased investment in healthcare robotsics, especially for surgery and rehabilitation. For instance, Bangkok Hospital is investing $5 million in a robotic surgery center. The University of Hong Kong is using robot-assisted spine surgery. Chinese and South Korean wearable rehabilitation robots are now available in the US and Australia. In addition, even happening in Asia area, a Chinese team recently created a small, highly sensitive biosensor for continuous glucose monitoring."
  },
  {
    "objectID": "posts/datah/papers/aspirin/index.html",
    "href": "posts/datah/papers/aspirin/index.html",
    "title": "Aspirin Vs Cancer",
    "section": "",
    "text": "The article:\n“It is a rather unexpected effect, because aspirin is mainly used as an anti-inflammatory drug,” says Marco Scarpa, a researcher at the University of Padova, and one of the authors of the study. As Scarpa notes, this study suggests that aspirin may be playing a slightly different role by stimulating the immune system’s surveillance response, which can then prevent or delay the progression of colorectal cancer.\nRead full article (opens in a new page)"
  },
  {
    "objectID": "posts/datah/papers/survival/index.html",
    "href": "posts/datah/papers/survival/index.html",
    "title": "Survival analysis",
    "section": "",
    "text": "Survival analysis\nIf you’re new to the world of medical research, you may have heard the term “survival analysis” thrown around and thought, “Sounds serious… is this about my career?” Don’t worry—it’s not (we hope!). Survival analysis is a statistical tool used to analyze time-to-event data, and it’s incredibly useful for understanding outcomes in medicine. Let’s break it down, step by step, with some examples.\nIn a nutshell, survival analysis answers questions like:\n\nHow long does it take for an event to occur?\nWhat factors influence the timing of that event?\n\nHere, the “event” could be a positive outcome (like recovery or a sneeze) or a less-than-ideal one (like disease progression or mortality). The key difference between survival analysis and other data analysis techniques is its ability to handle censored data. Censoring happens when you don’t observe the event during the study period—for example, a patient hasn’t died or relapsed by the time the study ends.\nFun Fact: Survival analysis isn’t just for medicine! It’s also used in engineering (how long before a machine breaks) and business (how long a customer stays loyal to a subscription). But in medicine, it’s a literal life-saver.\nCommon solution that Survival Analysis can gave us:\n\nCancer Treatment Trials\n\nImagine you’re testing a new chemotherapy drug. You want to know how long patients survive after starting treatment. Some patients may still be alive when the study ends, so you’ll have censored data. Survival analysis lets you account for this.\n\nOrgan Transplant Studies\n\nYou’re studying how long kidney transplants last before organ failure occurs. Here, time-to-event data (time until organ failure) is crucial to evaluating the success of different transplant techniques.\n\nTime to Readmission\n\nIf you’re analyzing how long patients stay out of the hospital after discharge, survival analysis helps identify risk factors for readmission and assess interventions to reduce it.\nCommon Survival Models (Pick Your Poison, Statistically Speaking)\n\nKaplan-Meier Curves\n\nThe Kaplan-Meier method is like the “intro course” to survival analysis. It estimates survival probabilities over time and creates a curve showing the proportion of patients surviving at different time points.\nWhy Use It?\n\nSimple and visual.\nGreat for comparing survival between groups (e.g., drug vs. placebo).\n\nExample: You’re evaluating survival after heart surgery in two groups: those who received a new device vs. standard care. Kaplan-Meier gives you a clear picture of how survival curves differ.\n\nCox Proportional Hazards Model\n\nThe Cox model takes survival analysis to the next level. It examines the relationship between survival time and multiple predictors (like age, smoking status, or treatment type). It’s called “proportional hazards” because it assumes the effect of predictors remains constant over time.\nWhy Use It?\n\nAdjusts for confounding variables.\nHelps identify which factors significantly impact survival.\n\nExample: In a cancer trial, the Cox model might reveal that tumor size and age significantly influence survival, while gender does not.\n\nParametric Models\n\nUnlike the Cox model, parametric models assume a specific distribution (e.g., exponential, Weibull) for survival times. These are great when you’re interested in modeling the actual shape of the survival curve.\nWhy Use It?\n\nMore informative when you can justify the assumptions.\nUseful for making predictions.\n\nExample: You’re studying time until remission in a chronic disease. A parametric model lets you estimate not just median survival but also probabilities at specific time points.\n\nCompeting Risks Models\n\nSometimes, patients can experience more than one type of event, and these events compete. For example, in a study on cancer patients, death from heart disease is a competing risk for death from cancer.\nWhy Use It?\n\nAccounts for competing risks, avoiding biased results.\n\nExample: You’re analyzing time to disease recurrence, but some patients die before recurrence occurs. Ignoring this competing risk would overestimate the recurrence risk.\nCommon Pitfalls when dealing with survival analysis (And How to Avoid Them)\n\nIgnoring Censoring\n\nCensoring is a cornerstone of survival analysis. Ignoring it is like ignoring patients who didn’t show up for their final follow-up—a big statistical no-no.\nPro Tip: Always check that your survival model properly accounts for censored data.\n\nOverfitting\n\nUsing too many variables in your model can lead to overfitting, where the model fits the data too closely and performs poorly on new datasets.\nPro Tip: Be parsimonious with predictors. Use stepwise selection or shrinkage techniques to avoid this.\n\nAssuming Proportional Hazards\n\nThe Cox model assumes that the hazard ratios are constant over time. If this assumption is violated, your results might be misleading.\nPro Tip: Check proportionality with diagnostic plots or use time-varying covariates if needed.\nSo, let’s Wrap It Up (Like a Survival Blanket). Why is Survival Analysis Useful?\nHandles Censored Data: Not everyone experiences the event during the study period, but survival analysis doesn’t leave their data out of the equation. It’s like giving everyone a voice, even if they didn’t “finish the race”.\nFlexible Models: Survival analysis offers a variety of models to suit your needs (more on that in a moment).\nClinical Insights: It helps you understand which variables (age, treatment type, comorbidities) affect time to an event, aiding personalized medicine.\nReal-Life Applications: Whether you’re studying disease progression, treatment durability, or hospital outcomes, survival analysis has your back.\nSurvival analysis isn’t as intimidating as it sounds. It’s an essential tool for MDs diving into clinical research, helping you understand and predict time-to-event outcomes. Whether you’re drawing Kaplan-Meier curves or diving into the depths of the Cox model, remember: survival analysis is all about making the most of your data, even when life (or research) gets complicated.\nSo, the next time someone asks, “How long until the event occurs?” you can confidently say, “Let me run a survival analysis on that!” If they ask what that means, feel free to share this article — or just tell them it’s not about your career. (We hope…)\nTo go deeper on the topic:\n\nKaplan EL, Meier P. “Nonparametric estimation from incomplete observations.” Journal of the American Statistical Association. 1958;53(282):457-481.\nCox DR. “Regression models and life-tables.” Journal of the Royal Statistical Society: Series B (Methodological). 1972;34(2):187-220.\nCollett D. Modelling Survival Data in Medical Research. Chapman & Hall/CRC, 2015.\nKleinbaum DG, Klein M. Survival Analysis: A Self-Learning Text. Springer, 2012.\nGoel MK, Khanna P, Kishore J. “Understanding survival analysis: Kaplan-Meier estimate.” International Journal of Ayurveda Research. 2010;1(4):274-278."
  },
  {
    "objectID": "tech.html",
    "href": "tech.html",
    "title": "Fresh Tech",
    "section": "",
    "text": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow Artificial Intelligence is improving the teaching journey in medical oncology\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow Novo Nordisk Accelerates drug development times from 12 weeks to 10 minutes with GenAI\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe world’s smallest surgical robot is among us\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT and his Artificial intelligence will make us live longer\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIs tech improving our healthcare journey?\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBitcoin like solutions and decentralized payments can help your healthcare’s balance\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nArtificial intelligence will improve your healthcare job routine\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/datah/medical_datactor/specializzazioni/index.html",
    "href": "posts/datah/medical_datactor/specializzazioni/index.html",
    "title": "How Artificial Intelligence is teaching medical oncology",
    "section": "",
    "text": "Artificial intelligence is entering classrooms to teach medical oncology. Italy is leading the way with the first study of its kind, called “AI Learning,” which will assess how well medical students learn when taught by AI-powered avatars. This innovative teaching platform, developed by the start-up ctcHealth, is named Plato. It offers an immersive and personalized educational experience and was recently unveiled at the Italian Summit on Precision Medicine, an international event organized by the Foundation for Personalized Medicine (FMP) in Rome, attended by over 150 experts.\nThe complexity of current diagnostic and therapeutic pathways requires highly specialized and tailored training, explains Paolo Marchetti, president of FMP and scientific director at IDI-IRCCS in Rome. Over the past fifty years, traditional teaching methods have seen very few changes, evolving from overhead projectors to electronic slides but without any real transformation in teaching practices. Plato represents a true leap forward, integrating advanced technology with a more interactive and engaging educational approach.\nThe research project initially focuses on oncology but aims to expand into other fields of medicine. Giuseppe Curigliano, president-elect of the European Society for Medical Oncology (ESMO), professor of medical oncology at the University of Milan, and director of the Division for New Drug Development for Innovative Therapies at the European Institute of Oncology (IEO) in Milan, believes that Plato has the potential to revolutionize medical education. Developed by a team of experts in education and artificial intelligence, it marks an important Italian contribution to the global landscape of advanced educational technologies. Plato may also be used to train members of Molecular Tumor Boards, multidisciplinary teams that are key players in the evolving model of precision oncology. The system is capable of analyzing genomic profiling data along with clinical information derived from precision oncology research within these boards.\nThe AI Learning study, promoted by FMP and hosted at Sapienza University of Rome, will start in September 2025. It will involve around 120 medical students in the final two years of their program.\nDomenico Alvaro, Dean of the Faculty of Medicine and Dentistry at Sapienza University, emphasizes the institution’s strong commitment to experimenting with innovative teaching strategies to enhance the education of future healthcare professionals. In a time marked by the digital and technological transformation of health sciences, it is crucial to prepare highly skilled professionals in new technologies. The educational project coordinated by Professor Botticelli, along with several initiatives led by Professor Marchetti, perfectly fits into this vision."
  },
  {
    "objectID": "posts/datah/medical_datactor/ai_onco/index.html",
    "href": "posts/datah/medical_datactor/ai_onco/index.html",
    "title": "How Artificial Intelligence is improving the teaching journey in medical oncology",
    "section": "",
    "text": "Artificial intelligence is entering classrooms to teach medical oncology. Italy is leading the way with the first study of its kind, called “AI Learning,” which will assess how well medical students learn when taught by AI-powered avatars. This innovative teaching platform, developed by the start-up “ctcHealth”, is named Plato. It offers an immersive and personalized educational experience and was recently unveiled at the Italian Summit on Precision Medicine, an international event organized by the Foundation for Personalized Medicine (FMP) in Rome, attended by over 150 experts.\nThe complexity of current diagnostic and therapeutic pathways requires highly specialized and tailored training, explains Paolo Marchetti, president of FMP and scientific director at IDI-IRCCS in Rome. Over the past fifty years, traditional teaching methods have seen very few changes, evolving from overhead projectors to electronic slides but without any real transformation in teaching practices. Plato represents a true leap forward, integrating advanced technology with a more interactive and engaging educational approach.\nThe research project initially focuses on oncology but aims to expand into other fields of medicine. Giuseppe Curigliano, president-elect of the European Society for Medical Oncology (ESMO), professor of medical oncology at the University of Milan, and director of the Division for New Drug Development for Innovative Therapies at the European Institute of Oncology (IEO) in Milan, believes that Plato has the potential to revolutionize medical education. Developed by a team of experts in education and artificial intelligence, it marks an important Italian contribution to the global landscape of advanced educational technologies. Plato may also be used to train members of Molecular Tumor Boards, multidisciplinary teams that are key players in the evolving model of precision oncology. The system is capable of analyzing genomic profiling data along with clinical information derived from precision oncology research within these boards.\nThe AI Learning study, promoted by FMP and hosted at Sapienza University of Rome, will start in September 2025. It will involve around 120 medical students in the final two years of their program.\nDomenico Alvaro, Dean of the Faculty of Medicine and Dentistry at Sapienza University, emphasizes the institution’s strong commitment to experimenting with innovative teaching strategies to enhance the education of future healthcare professionals. In a time marked by the digital and technological transformation of health sciences, it is crucial to prepare highly skilled professionals in new technologies. The educational project coordinated by Professor Botticelli, along with several initiatives led by Professor Marchetti, perfectly fits into this vision."
  },
  {
    "objectID": "posts/datah/medical_datactor/ai_pharma/index.html",
    "href": "posts/datah/medical_datactor/ai_pharma/index.html",
    "title": "How Novo Nordisk Accelerates drug development times from 12 weeks to 10 minutes with GenAI",
    "section": "",
    "text": "Novo Nordisk, a global leader in diabetes and chronic disease care, has reimagined the way it creates clinical study reports, by using MongoDB Atlas together with Amazon Bedrock. Traditionally, compiling a clinical study reports took around 12 weeks and required collaboration across teams of statisticians, scientists, and technical writers. The process was slow and often delayed both regulatory approval and the delivery of new treatments to patients.\nIn 2023, Novo Nordisk introduced NovoScribe, a new platform that automates much of the CSR writing process. NovoScribe uses retrieval-augmented generation combined with large language models like Claude 3, Titan, and a private version of ChatGPT. MongoDB Atlas plays a central role by managing the structured data that powers the system. By integrating clinical trial data with smart templates, the platform helps the AI models produce complete and accurate reports in just 10 minutes.\nFrom such investments, Novo Nordisk has become the first pharmaceutical company to automate CSR generation at scale. Faster reporting not only helps the company accelerate regulatory submissions but also means patients can access new therapies more quickly. It is a major shift in how the pharmaceutical industry approaches reporting and data management.\nSource: https://www.mongodb.com/solutions/customer-case-studies/novo-nordisk"
  },
  {
    "objectID": "posts/datah/papers/p_value/index.html",
    "href": "posts/datah/papers/p_value/index.html",
    "title": "Your P-value is much more than you think",
    "section": "",
    "text": "The p-value is a tool used to interpret statistical tests. It represents the probability that the observed results could have occurred under the assumption that the null hypothesis is true. In other words, it estimates the likelihood that what we are claiming is correct, within a small margin of error.\nWhen comparing two treatments, like the classical treatment A and treatment B, the whole work starts with the assumption of the null hypothesis: that there is no difference between the two. The alternative hypothesis so that there is a difference and can be accepted if the null hypothesis is rejected.\nIn a nutshell statistical significance reflects how likely it is that the observed difference is real. There can never be absolute certainty, as a predefined margin of error must always be taken into account.\nConventionally, a threshold of p &lt; 0.05 like a 5% margin of error is used to define statistical significance. This means there is less than a 5 in 100 chance that Treatment A would appear more effective than Treatment B purely by random chance.\n\nIf p &gt; 0.05: no statistically significant difference (null hypothesis is not rejected).\nIf p &lt; 0.05: statistically significant difference (null hypothesis is rejected)."
  },
  {
    "objectID": "posts/datah/papers/p_value/index.html#lets-start-what-does-the-p-value-depicts",
    "href": "posts/datah/papers/p_value/index.html#lets-start-what-does-the-p-value-depicts",
    "title": "Your P-value is much more than you think",
    "section": "",
    "text": "The p-value is a tool used to interpret statistical tests. It represents the probability that the observed results could have occurred under the assumption that the null hypothesis is true. In other words, it estimates the likelihood that what we are claiming is correct, within a small margin of error.\nWhen comparing two treatments, like the classical treatment A and treatment B, the whole work starts with the assumption of the null hypothesis: that there is no difference between the two. The alternative hypothesis so that there is a difference and can be accepted if the null hypothesis is rejected.\nIn a nutshell statistical significance reflects how likely it is that the observed difference is real. There can never be absolute certainty, as a predefined margin of error must always be taken into account.\nConventionally, a threshold of p &lt; 0.05 like a 5% margin of error is used to define statistical significance. This means there is less than a 5 in 100 chance that Treatment A would appear more effective than Treatment B purely by random chance.\n\nIf p &gt; 0.05: no statistically significant difference (null hypothesis is not rejected).\nIf p &lt; 0.05: statistically significant difference (null hypothesis is rejected)."
  },
  {
    "objectID": "posts/datah/papers/p_value/index.html#why-is-p-0.05-considered-statistically-significant",
    "href": "posts/datah/papers/p_value/index.html#why-is-p-0.05-considered-statistically-significant",
    "title": "Your P-value is much more than you think",
    "section": "Why is p < 0.05 considered statistically significant?",
    "text": "Why is p &lt; 0.05 considered statistically significant?\nThe 0.05 cut-off is a conventional threshold. It dates back to 1926 when R.A. Fisher wrote:\n\n“It is convenient to draw a line at about the level at which we can say: ‘Either there is something in the treatment, or a coincidence has occurred such as does not occur more than once in twenty trials.’”\n\nHowever, Fisher later clarified:\n\n“No scientific worker has a fixed level of significance at which from every experiment and in all circumstances he rejects hypotheses.”"
  },
  {
    "objectID": "posts/datah/papers/p_value/index.html#the-limitations-of-a-fixed-0.05-threshold-the-winlose-fallacy",
    "href": "posts/datah/papers/p_value/index.html#the-limitations-of-a-fixed-0.05-threshold-the-winlose-fallacy",
    "title": "Your P-value is much more than you think",
    "section": "The limitations of a fixed 0.05 threshold: the win/lose fallacy",
    "text": "The limitations of a fixed 0.05 threshold: the win/lose fallacy\nThe threshold is often misinterpreted as a strict decision rule:\n\np &lt; 0.05: you “win”\np &gt; 0.05: you “lose”\n\nThis dichotomous view is misleading both statistically and clinically. Small changes in sample size or data can push a p-value above or below 0.05 without reflecting any real difference in effect size or clinical relevance. This shows that:\n\np-values are sensitive to sample size.\nThey depend heavily on study design and data quality."
  },
  {
    "objectID": "posts/datah/papers/p_value/index.html#key-takeaways",
    "href": "posts/datah/papers/p_value/index.html#key-takeaways",
    "title": "Your P-value is much more than you think",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe p-value is not a measure of the truth of the hypothesis.\nScientific conclusions should not rely solely on whether p is above or below 0.05.\nThe p-value does not indicate the size or importance of an effect.\nA p-value, taken out of context, is not a good measure of evidence."
  },
  {
    "objectID": "posts/datah/papers/p_value/index.html#a-better-approach-to-interpretation",
    "href": "posts/datah/papers/p_value/index.html#a-better-approach-to-interpretation",
    "title": "Your P-value is much more than you think",
    "section": "A Better Approach to Interpretation",
    "text": "A Better Approach to Interpretation\nInstead of relying on a dichotomized hypothesis test based on a fixed p-value threshold, a more nuanced method should be used, incorporating:\n\nEffect estimates like relative risk, odds ratio, hazard ratio\nAbsolute measures like absolute risk, number needed to treat – NNT\nUncertainty estimates like confidence intervals\nP-values, in context\n\nThis allows for informed, inferential reasoning by clinicians and statisticians to assess the scientific and clinical significance of findings."
  },
  {
    "objectID": "posts/datah/papers/p_value/index.html#example-the-elan-study",
    "href": "posts/datah/papers/p_value/index.html#example-the-elan-study",
    "title": "Your P-value is much more than you think",
    "section": "Example: The ELAN Study",
    "text": "Example: The ELAN Study\nThe ELAN (Early versus Later Anticoagulation for Stroke with Atrial Fibrillation) study was a randomized controlled trial involving 2,013 patients with recent ischemic stroke and atrial fibrillation.\nPatients were randomized to: - Early anticoagulation: within 48 hours (minor/moderate stroke) or day 6–7 (major stroke). - Delayed anticoagulation: day 3–4 (minor), day 6–7 (moderate), or day 12–14 (major).\n\nPrimary Endpoint\nA composite of ischemic stroke, systemic embolism, bleeding, symptomatic hemorrhage, or vascular death within 30 days.\n\nEarly group: 2.9% experienced the event\nDelayed group: 4.1%\nAbsolute risk difference: -1.18 percentage points (95% CI: -2.84 to 0.47)\nOdds of ischemic stroke recurrence were nearly halved (OR 0.57; 95% CI: 0.29 to 1.07)\n\nAt 90 days, the primary outcome was also lower in the early treatment group: - OR 0.65; 95% CI: 0.42 to 0.99\n\n\nFinal interpretation\nThe study design did not include formal superiority or non-inferiority testing. Instead, it used descriptive statistics with confidence intervals, not relying on the p-value alone. This provides clinicians with useful context: the 30-day risk difference may range from a 2.8% reduction to a 0.5% increase, helping inform decisions about when to restart anti coagulation therapy."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html",
    "href": "posts/datah/papers/standard dev/index.html",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Here we are with the big match between standard deviation (SD) and standard error (SE).\nOK, let’s say it their names sound similar and both involve measuring variability, they serve fundamentally different purposes in statistical analysis. Understanding this difference is crucial for interpreting research findings correctly and avoiding common misinterpretations in biomedical literature.\n\n\nStandard deviation measures the spread of individual observations around the mean within a single sample. It quantifies how much individual data points deviate from the average value in your dataset.\n\n\n\nDescribes the data itself: SD tells you about the natural variability present in your sample\nUnits: Expressed in the same units as your original measurements\nSample size independence: Generally remains stable regardless of sample size (assuming you’re sampling from the same population)\nPopulation parameter: Estimates the true population standard deviation (σ)\n\n\n\n\nSuppose you measure systolic blood pressure in 100 patients and find a mean of 130 mmHg with an SD of 15 mmHg. This tells you that most patients’ blood pressure readings fall within about 15 mmHg of the average—some patients might have readings around 115 mmHg, others around 145 mmHg. The SD describes this natural biological variation in blood pressure among individuals.\n\n\n\n\nStandard error measures how precisely you’ve estimated a population parameter (like the mean) based on your sample. It quantifies how much your sample statistic would vary if you repeated the study multiple times with different samples from the same population.\n\n\n\nDescribes estimation precision: SE tells you about the reliability of your sample statistic\nUnits: Same as the original measurements, but conceptually different meaning\nSample size dependent: Gets smaller as sample size increases (SE = SD/√n)\nSampling distribution: Relates to the theoretical distribution of sample means\n\n\n\n\nUsing the same blood pressure study, if your SE is 1.5 mmHg, this means that if you repeated the study many times with different groups of 100 patients, about 68% of your sample means would fall within 1.5 mmHg of the true population mean. The smaller the SE, the more confident you can be that your sample mean is close to the true population mean.\n\n\n\n\nWhile this guide focuses on conceptual understanding, the relationship between SD and SE is straightforward:\nSE = SD ÷ √(sample size)\nThis formula reveals why SE decreases as sample size increases—you’re dividing by a larger number. However, SD typically remains relatively constant across different sample sizes from the same population.\n\n\n\n\n\n\nDescribing your sample: “The patients had diverse responses, with individual scores ranging widely (SD = 12 points)”\nClinical interpretation: Understanding the range of individual patient outcomes\nAssessing biological variability: Showing how much individuals differ from each other\nSample characteristics: Describing what you actually observed in your study\n\n\n\n\n\nEstimating precision: “We can be confident our estimate is accurate (SE = 0.8)”\nStatistical inference: Calculating confidence intervals and p-values\nComparing studies: Evaluating how reliable different estimates are\nMeta-analyses: Weighing studies based on their precision\n\n\n\n\n\n\n\nSome researchers inappropriately report SE instead of SD because SE values are always smaller, making results appear more precise than they actually are. This is misleading because it doesn’t accurately represent the variability in the actual data.\n\n\n\nSE doesn’t tell you about the spread of individual observations. A small SE doesn’t mean your patients had similar outcomes—it means you estimated the average outcome precisely.\n\n\n\nUnlike SE, SD doesn’t necessarily get smaller with larger sample sizes. If you’re sampling from the same population, SD should remain relatively stable regardless of whether you have 50 or 500 participants.\n\n\n\n\n\n\n\nSD perspective: “Individual patients showed varied responses to the drug, with some improving dramatically and others showing little change (SD = 8.5 points on the symptom scale)”\nSE perspective: “We can be confident that the average drug effect is between 12-16 points improvement (mean = 14, SE = 1.0)”\n\n\n\n\n\nSD perspective: “Normal glucose levels in healthy adults range widely due to individual biological differences (mean = 90 mg/dL, SD = 10 mg/dL)”\nSE perspective: “Our estimate of the population mean glucose level is quite precise (SE = 0.8 mg/dL based on 156 participants)”\n\n\n\n\n\nWhen you see error bars in research papers: - SD error bars: Show the spread of individual data points - SE error bars: Show the precision of the mean estimate - 95% CI error bars: Show the range likely to contain the true population mean\nAlways check figure legends to understand which type of error bar is being used, as this dramatically affects interpretation.\n\n\n\nUnderstanding the SD vs SE distinction is crucial for proper statistical analysis:\n\nt-tests and confidence intervals: Use SE for calculations\nSample size planning: Consider both the expected SD (effect size) and desired SE (precision)\nPower analysis: Requires understanding of population SD to estimate SE for different sample sizes\n\n\n\n\nProfessional medical journals typically require: - Descriptive statistics: Report means with SD to characterize your sample - Inferential statistics: Report means with SE or confidence intervals when making population inferences - Clear labeling: Always specify whether you’re reporting SD, SE, or CI\n\n\n\nThink of SD as describing “what you found” in your sample, while SE describes “how sure you can be” about what that finding means for the broader population. Both are essential, but for different purposes in the research process.\nRemember: Good statistical practice involves reporting the right measure for your intended message. Use SD to help readers understand your data, and SE to help them understand your conclusions."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#what-is-standard-deviation-sd",
    "href": "posts/datah/papers/standard dev/index.html#what-is-standard-deviation-sd",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Standard deviation measures the spread of individual observations around the mean within a single sample. It quantifies how much individual data points deviate from the average value in your dataset.\n\n\n\nDescribes the data itself: SD tells you about the natural variability present in your sample\nUnits: Expressed in the same units as your original measurements\nSample size independence: Generally remains stable regardless of sample size (assuming you’re sampling from the same population)\nPopulation parameter: Estimates the true population standard deviation (σ)\n\n\n\n\nSuppose you measure systolic blood pressure in 100 patients and find a mean of 130 mmHg with an SD of 15 mmHg. This tells you that most patients’ blood pressure readings fall within about 15 mmHg of the average—some patients might have readings around 115 mmHg, others around 145 mmHg. The SD describes this natural biological variation in blood pressure among individuals."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#what-is-standard-error-se",
    "href": "posts/datah/papers/standard dev/index.html#what-is-standard-error-se",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Standard error measures how precisely you’ve estimated a population parameter (like the mean) based on your sample. It quantifies how much your sample statistic would vary if you repeated the study multiple times with different samples from the same population.\n\n\n\nDescribes estimation precision: SE tells you about the reliability of your sample statistic\nUnits: Same as the original measurements, but conceptually different meaning\nSample size dependent: Gets smaller as sample size increases (SE = SD/√n)\nSampling distribution: Relates to the theoretical distribution of sample means\n\n\n\n\nUsing the same blood pressure study, if your SE is 1.5 mmHg, this means that if you repeated the study many times with different groups of 100 patients, about 68% of your sample means would fall within 1.5 mmHg of the true population mean. The smaller the SE, the more confident you can be that your sample mean is close to the true population mean."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#the-mathematical-relationship",
    "href": "posts/datah/papers/standard dev/index.html#the-mathematical-relationship",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "While this guide focuses on conceptual understanding, the relationship between SD and SE is straightforward:\nSE = SD ÷ √(sample size)\nThis formula reveals why SE decreases as sample size increases—you’re dividing by a larger number. However, SD typically remains relatively constant across different sample sizes from the same population."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#when-to-use-each-measure",
    "href": "posts/datah/papers/standard dev/index.html#when-to-use-each-measure",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Describing your sample: “The patients had diverse responses, with individual scores ranging widely (SD = 12 points)”\nClinical interpretation: Understanding the range of individual patient outcomes\nAssessing biological variability: Showing how much individuals differ from each other\nSample characteristics: Describing what you actually observed in your study\n\n\n\n\n\nEstimating precision: “We can be confident our estimate is accurate (SE = 0.8)”\nStatistical inference: Calculating confidence intervals and p-values\nComparing studies: Evaluating how reliable different estimates are\nMeta-analyses: Weighing studies based on their precision"
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#common-mistakes-and-misconceptions",
    "href": "posts/datah/papers/standard dev/index.html#common-mistakes-and-misconceptions",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Some researchers inappropriately report SE instead of SD because SE values are always smaller, making results appear more precise than they actually are. This is misleading because it doesn’t accurately represent the variability in the actual data.\n\n\n\nSE doesn’t tell you about the spread of individual observations. A small SE doesn’t mean your patients had similar outcomes—it means you estimated the average outcome precisely.\n\n\n\nUnlike SE, SD doesn’t necessarily get smaller with larger sample sizes. If you’re sampling from the same population, SD should remain relatively stable regardless of whether you have 50 or 500 participants."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#practical-examples-in-biomedical-research",
    "href": "posts/datah/papers/standard dev/index.html#practical-examples-in-biomedical-research",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "SD perspective: “Individual patients showed varied responses to the drug, with some improving dramatically and others showing little change (SD = 8.5 points on the symptom scale)”\nSE perspective: “We can be confident that the average drug effect is between 12-16 points improvement (mean = 14, SE = 1.0)”\n\n\n\n\n\nSD perspective: “Normal glucose levels in healthy adults range widely due to individual biological differences (mean = 90 mg/dL, SD = 10 mg/dL)”\nSE perspective: “Our estimate of the population mean glucose level is quite precise (SE = 0.8 mg/dL based on 156 participants)”"
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#visual-understanding-error-bars-in-graphs",
    "href": "posts/datah/papers/standard dev/index.html#visual-understanding-error-bars-in-graphs",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "When you see error bars in research papers: - SD error bars: Show the spread of individual data points - SE error bars: Show the precision of the mean estimate - 95% CI error bars: Show the range likely to contain the true population mean\nAlways check figure legends to understand which type of error bar is being used, as this dramatically affects interpretation."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#impact-on-statistical-testing",
    "href": "posts/datah/papers/standard dev/index.html#impact-on-statistical-testing",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Understanding the SD vs SE distinction is crucial for proper statistical analysis:\n\nt-tests and confidence intervals: Use SE for calculations\nSample size planning: Consider both the expected SD (effect size) and desired SE (precision)\nPower analysis: Requires understanding of population SD to estimate SE for different sample sizes"
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#reporting-guidelines",
    "href": "posts/datah/papers/standard dev/index.html#reporting-guidelines",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Professional medical journals typically require: - Descriptive statistics: Report means with SD to characterize your sample - Inferential statistics: Report means with SE or confidence intervals when making population inferences - Clear labeling: Always specify whether you’re reporting SD, SE, or CI"
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#key-takeaway",
    "href": "posts/datah/papers/standard dev/index.html#key-takeaway",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Think of SD as describing “what you found” in your sample, while SE describes “how sure you can be” about what that finding means for the broader population. Both are essential, but for different purposes in the research process.\nRemember: Good statistical practice involves reporting the right measure for your intended message. Use SD to help readers understand your data, and SE to help them understand your conclusions."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#what-is-standard-error",
    "href": "posts/datah/papers/standard dev/index.html#what-is-standard-error",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Standard error measures how precisely you’ve estimated a population parameter (like the mean) based on your sample. It quantifies how much your sample statistic would vary if you repeated the study multiple times with different samples from the same population.\n\n\n\nDescribes estimation precision: SE tells you about the reliability of your sample statistic\nUnits: Same as the original measurements, but conceptually different meaning\nSample size dependent: Gets smaller as sample size increases (SE = SD/√n)\nSampling distribution: Relates to the theoretical distribution of sample means\n\n\n\n\nUsing the same blood pressure study, if your SE is 1.5 mmHg, this means that if you repeated the study many times with different groups of 100 patients, about 68% of your sample means would fall within 1.5 mmHg of the true population mean. The smaller the SE, the more confident you can be that your sample mean is close to the true population mean."
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#when-to-shine-with-each-measure",
    "href": "posts/datah/papers/standard dev/index.html#when-to-shine-with-each-measure",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Describing your sample: “The patients had diverse responses, with individual scores ranging widely (SD = 12 points)”\nClinical interpretation: Understanding the range of individual patient outcomes\nAssessing biological variability: Showing how much individuals differ from each other\nSample characteristics: Describing what you actually observed in your study\n\n\n\n\n\nEstimating precision: “We can be confident our estimate is accurate (SE = 0.8)”\nStatistical inference: Calculating confidence intervals and p-values\nComparing studies: Evaluating how reliable different estimates are\nMeta-analyses: Weighing studies based on their precision"
  },
  {
    "objectID": "posts/datah/papers/standard dev/index.html#fancy-takeaway",
    "href": "posts/datah/papers/standard dev/index.html#fancy-takeaway",
    "title": "Standard Deviation Vs Standard Error",
    "section": "",
    "text": "Think of SD as describing “what you found” in your sample, while SE describes “how sure you can be” about what that finding means for the broader population. Both are essential, but for different purposes in the research process.\nRemember: Good statistical practice involves reporting the right measure for your intended message. Use SD to help readers understand your data, and SE to help them understand your conclusions."
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html",
    "href": "posts/datah/papers/regression_result/index.html",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "In this article you’ll see the way to communicate to your stakeholders the way to present the results in a clear, accessible format. The gtsummary package transform complex statistical output into publication-ready tables that speak to any audience.\n\n\nTraditional R output, usually, presents several challenges when communicating with broader audiences:\n\n\n\nInformation overload: Raw output includes MANY technical details (residuals, diagnostic statistics, model fit information) that can overwhelm your stakeholders\nPoor formatting: Console output lacks visual hierarchy and professional appearance\nTechnical jargon: Terms like “Std. Error,” “t value,” and “Pr(&gt;|t|)” require statistical background to interpret\nInconsistent presentation: Different model types produce different output formats, making comparisons difficult\n\n\n\n\nImagine presenting regression results from a clinical trial to a medical advisory board. Raw R output might show:\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         45.2341     2.1234   21.29  &lt; 2e-16 ***\ntreatment_group      8.7654     1.5678    5.59  8.9e-07 ***\nage                  0.2345     0.0876    2.68   0.0089 ** \ngender_female       -2.1098     1.2345   -1.71   0.0912 .  \nWhile statisticians can quickly interpret this, clinicians need to know: “What does this mean for patient outcomes?”\n\n\n\n\ngtsummary is an R package that creates elegant, publication-ready summary tables from statistical models. It automatically formats results, adds appropriate labels, and presents findings in a way that’s immediately interpretable by diverse audiences.\n\n\n\nClarity over complexity: Present only the most relevant information\nAudience-appropriate: Adapt technical results for non-technical stakeholders\nProfessional appearance: Generate tables ready for publications, presentations, and reports\nConsistency: Standardize output across different types of analyses\n\n\n\n\n\n\n\nThe tbl_regression() function transforms raw regression output into clean, interpretable tables.\nKey features: - Automatic formatting of coefficients and confidence intervals - Clear variable labeling - Optional p-values with appropriate formatting - Customizable precision and display options\nExample transformation: Instead of raw coefficient output, you get a table showing: - Variable: Treatment Group - Coefficient: 8.77 (95% CI: 5.70, 11.84) - p-value: &lt;0.001\n\n\n\nCreates comprehensive descriptive statistics tables that are immediately publication-ready.\nApplications: - Baseline characteristics: Compare treatment groups in clinical trials - Population descriptions: Summarize study participant characteristics - Stratified analyses: Show results by subgroups\n\n\n\nTransforms complex survival analysis output into clear, interpretable tables showing: - Median survival times with confidence intervals - Survival probabilities at key time points - Hazard ratios with clear interpretation\n\n\n\n\n\n\nLinear Models: - Simple and multiple linear regression - Analysis of variance (ANOVA) - Analysis of covariance (ANCOVA)\nGeneralized Linear Models: - Logistic regression (odds ratios automatically calculated) - Poisson regression (incidence rate ratios) - Negative binomial regression\nAdvanced Models: - Cox proportional hazards models - Mixed-effects models (with appropriate packages) - Bayesian models (with brms integration)\nSpecialized Applications: - Dose-response analyses - Propensity score matching results - Meta-analysis summaries\n\n\n\nRaw R output shows log-odds coefficients that are difficult to interpret:\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)\ntreatment       1.2345     0.3456    3.57   0.0004\ngtsummary automatically converts this to odds ratios: - Treatment: OR = 3.44 (95% CI: 1.75, 6.77), p &lt; 0.001\nThis immediately tells clinicians that treatment increases the odds of success by 244%.\n\n\n\n\n\n\n\nEmphasize clinical significance: Include effect sizes and confidence intervals\nPlain language labels: Replace variable names with descriptive text\nRelevant precision: Show appropriate decimal places for clinical context\n\n\n\n\n\nComplete statistical information: Include all required statistics\nStandardized formatting: Follow regulatory guidelines for table presentation\nFootnote integration: Add necessary disclaimers and explanations\n\n\n\n\n\nSimplified display: Focus on key results only\nVisual emphasis: Highlight significant findings\nContext provision: Include baseline comparisons\n\n\n\n\n\n\n\n\nConfidence interval levels: Adjust from default 95% to other levels\nP-value formatting: Control decimal places and significance indicators\nEffect size measures: Include standardized coefficients or effect sizes\n\n\n\n\n\nConditional formatting: Highlight significant results\nCustom themes: Match organizational branding\nIntegration capabilities: Export to Word, HTML, or LaTeX\n\n\n\n\n\nModel comparison tables: Compare multiple models side-by-side\nStratified analyses: Present results by subgroups\nCombined results: Merge different types of analyses\n\n\n\n\n\n\n\n\nStatistical background: Adjust complexity accordingly\nDomain expertise: Use appropriate terminology\nDecision-making needs: Highlight actionable results\n\n\n\n\n\nLogical organization: Group related variables together\nClear headers: Use descriptive column names\nAppropriate precision: Match decimal places to measurement precision\nConsistent formatting: Standardize across all tables\n\n\n\n\n\nFootnotes: Explain statistical terms when necessary\nReference categories: Clearly identify comparison groups\nClinical context: Include baseline values or normal ranges\n\n\n\n\n\n\n\nProblem: R variable names (e.g., trt_grp, age_yrs) aren’t presentation-ready Solution: Use descriptive labels (“Treatment Group”, “Age (years)”)\n\n\n\nProblem: Comparing results across different model types Solution: Standardize presentation format across all models\n\n\n\nProblem: Interaction terms are difficult to present clearly Solution: Consider stratified analyses or graphical presentations alongside tables\n\n\n\n\n\n\n\nReproducibility: Tables update automatically when data changes\nVersion control: Track changes in presentation over time\nCollaboration: Standardized output facilitates team communication\nQuality control: Reduces manual formatting errors\n\n\n\n\n\nAudit trail: Clear connection between analysis code and presentation\nTransparency: Analysis decisions are explicit in code\nEfficiency: Automated formatting saves time and reduces errors\n\n\n\n\n\n\n\n\nIncreased impact: Clear presentations lead to better understanding\nTime savings: Automated formatting reduces manual work\nProfessional appearance: Publication-ready output enhances credibility\n\n\n\n\n\nBetter understanding: Clear tables facilitate informed decisions\nFaster review: Well-organized results speed up evaluation process\nReduced miscommunication: Standardized presentation prevents misinterpretation\n\n\n\n\n\nImproved standards: Elevates expectations for result presentation\nBetter science communication: Bridges gap between analysis and application\nEnhanced reproducibility: Standardized approaches improve consistency\n\n\n\n\n\n\n\n\nInteractive tables: Integration with web-based presentation tools\nAutomated interpretation: AI-assisted result explanation\nPersonalized presentation: Audience-specific automatic formatting\n\n\n\n\n\nThe transition from raw statistical output to professional presentation isn’t just about aesthetics—it’s about effective scientific communication. Tools like gtsummary transform complex analyses into accessible insights, ensuring that statistical findings can inform decision-making at all levels.\nIn an era where evidence-based decision making is crucial, the ability to communicate statistical results clearly and professionally has become as important as the analysis itself. By investing in proper presentation tools and techniques, researchers can maximize the impact of their work and ensure that valuable insights reach and influence their intended audiences.\nRemember: Great statistics poorly communicated are far less valuable than good statistics clearly presented. The goal is not just to analyze data, but to transform that analysis into actionable knowledge."
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#the-problem-with-raw-statistical-output",
    "href": "posts/datah/papers/regression_result/index.html#the-problem-with-raw-statistical-output",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Traditional R output, usually, presents several challenges when communicating with broader audiences:\n\n\n\nInformation overload: Raw output includes MANY technical details (residuals, diagnostic statistics, model fit information) that can overwhelm your stakeholders\nPoor formatting: Console output lacks visual hierarchy and professional appearance\nTechnical jargon: Terms like “Std. Error,” “t value,” and “Pr(&gt;|t|)” require statistical background to interpret\nInconsistent presentation: Different model types produce different output formats, making comparisons difficult\n\n\n\n\nImagine presenting regression results from a clinical trial to a medical advisory board. Raw R output might show:\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         45.2341     2.1234   21.29  &lt; 2e-16 ***\ntreatment_group      8.7654     1.5678    5.59  8.9e-07 ***\nage                  0.2345     0.0876    2.68   0.0089 ** \ngender_female       -2.1098     1.2345   -1.71   0.0912 .  \nWhile statisticians can quickly interpret this, clinicians need to know: “What does this mean for patient outcomes?”"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#what-is-gtsummary",
    "href": "posts/datah/papers/regression_result/index.html#what-is-gtsummary",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "gtsummary is an R package that creates elegant, publication-ready summary tables from statistical models. It automatically formats results, adds appropriate labels, and presents findings in a way that’s immediately interpretable by diverse audiences.\n\n\n\nClarity over complexity: Present only the most relevant information\nAudience-appropriate: Adapt technical results for non-technical stakeholders\nProfessional appearance: Generate tables ready for publications, presentations, and reports\nConsistency: Standardize output across different types of analyses"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#key-functions-and-applications",
    "href": "posts/datah/papers/regression_result/index.html#key-functions-and-applications",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "The tbl_regression() function transforms raw regression output into clean, interpretable tables.\nKey features: - Automatic formatting of coefficients and confidence intervals - Clear variable labeling - Optional p-values with appropriate formatting - Customizable precision and display options\nExample transformation: Instead of raw coefficient output, you get a table showing: - Variable: Treatment Group - Coefficient: 8.77 (95% CI: 5.70, 11.84) - p-value: &lt;0.001\n\n\n\nCreates comprehensive descriptive statistics tables that are immediately publication-ready.\nApplications: - Baseline characteristics: Compare treatment groups in clinical trials - Population descriptions: Summarize study participant characteristics - Stratified analyses: Show results by subgroups\n\n\n\nTransforms complex survival analysis output into clear, interpretable tables showing: - Median survival times with confidence intervals - Survival probabilities at key time points - Hazard ratios with clear interpretation"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#model-versatility-beyond-linear-regression",
    "href": "posts/datah/papers/regression_result/index.html#model-versatility-beyond-linear-regression",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Linear Models: - Simple and multiple linear regression - Analysis of variance (ANOVA) - Analysis of covariance (ANCOVA)\nGeneralized Linear Models: - Logistic regression (odds ratios automatically calculated) - Poisson regression (incidence rate ratios) - Negative binomial regression\nAdvanced Models: - Cox proportional hazards models - Mixed-effects models (with appropriate packages) - Bayesian models (with brms integration)\nSpecialized Applications: - Dose-response analyses - Propensity score matching results - Meta-analysis summaries\n\n\n\nRaw R output shows log-odds coefficients that are difficult to interpret:\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)\ntreatment       1.2345     0.3456    3.57   0.0004\ngtsummary automatically converts this to odds ratios: - Treatment: OR = 3.44 (95% CI: 1.75, 6.77), p &lt; 0.001\nThis immediately tells clinicians that treatment increases the odds of success by 244%."
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#customization-for-different-audiences",
    "href": "posts/datah/papers/regression_result/index.html#customization-for-different-audiences",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Emphasize clinical significance: Include effect sizes and confidence intervals\nPlain language labels: Replace variable names with descriptive text\nRelevant precision: Show appropriate decimal places for clinical context\n\n\n\n\n\nComplete statistical information: Include all required statistics\nStandardized formatting: Follow regulatory guidelines for table presentation\nFootnote integration: Add necessary disclaimers and explanations\n\n\n\n\n\nSimplified display: Focus on key results only\nVisual emphasis: Highlight significant findings\nContext provision: Include baseline comparisons"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#advanced-features",
    "href": "posts/datah/papers/regression_result/index.html#advanced-features",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Confidence interval levels: Adjust from default 95% to other levels\nP-value formatting: Control decimal places and significance indicators\nEffect size measures: Include standardized coefficients or effect sizes\n\n\n\n\n\nConditional formatting: Highlight significant results\nCustom themes: Match organizational branding\nIntegration capabilities: Export to Word, HTML, or LaTeX\n\n\n\n\n\nModel comparison tables: Compare multiple models side-by-side\nStratified analyses: Present results by subgroups\nCombined results: Merge different types of analyses"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#best-practices-for-implementation",
    "href": "posts/datah/papers/regression_result/index.html#best-practices-for-implementation",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Statistical background: Adjust complexity accordingly\nDomain expertise: Use appropriate terminology\nDecision-making needs: Highlight actionable results\n\n\n\n\n\nLogical organization: Group related variables together\nClear headers: Use descriptive column names\nAppropriate precision: Match decimal places to measurement precision\nConsistent formatting: Standardize across all tables\n\n\n\n\n\nFootnotes: Explain statistical terms when necessary\nReference categories: Clearly identify comparison groups\nClinical context: Include baseline values or normal ranges"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#common-implementation-challenges",
    "href": "posts/datah/papers/regression_result/index.html#common-implementation-challenges",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Problem: R variable names (e.g., trt_grp, age_yrs) aren’t presentation-ready Solution: Use descriptive labels (“Treatment Group”, “Age (years)”)\n\n\n\nProblem: Comparing results across different model types Solution: Standardize presentation format across all models\n\n\n\nProblem: Interaction terms are difficult to present clearly Solution: Consider stratified analyses or graphical presentations alongside tables"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#integration-with-reproducible-research",
    "href": "posts/datah/papers/regression_result/index.html#integration-with-reproducible-research",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Reproducibility: Tables update automatically when data changes\nVersion control: Track changes in presentation over time\nCollaboration: Standardized output facilitates team communication\nQuality control: Reduces manual formatting errors\n\n\n\n\n\nAudit trail: Clear connection between analysis code and presentation\nTransparency: Analysis decisions are explicit in code\nEfficiency: Automated formatting saves time and reduces errors"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#impact-on-statistical-communication",
    "href": "posts/datah/papers/regression_result/index.html#impact-on-statistical-communication",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Increased impact: Clear presentations lead to better understanding\nTime savings: Automated formatting reduces manual work\nProfessional appearance: Publication-ready output enhances credibility\n\n\n\n\n\nBetter understanding: Clear tables facilitate informed decisions\nFaster review: Well-organized results speed up evaluation process\nReduced miscommunication: Standardized presentation prevents misinterpretation\n\n\n\n\n\nImproved standards: Elevates expectations for result presentation\nBetter science communication: Bridges gap between analysis and application\nEnhanced reproducibility: Standardized approaches improve consistency"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#future-considerations",
    "href": "posts/datah/papers/regression_result/index.html#future-considerations",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "Interactive tables: Integration with web-based presentation tools\nAutomated interpretation: AI-assisted result explanation\nPersonalized presentation: Audience-specific automatic formatting"
  },
  {
    "objectID": "posts/datah/papers/regression_result/index.html#key-takeaway",
    "href": "posts/datah/papers/regression_result/index.html#key-takeaway",
    "title": "Easies way to show your statistical output to everybody",
    "section": "",
    "text": "The transition from raw statistical output to professional presentation isn’t just about aesthetics—it’s about effective scientific communication. Tools like gtsummary transform complex analyses into accessible insights, ensuring that statistical findings can inform decision-making at all levels.\nIn an era where evidence-based decision making is crucial, the ability to communicate statistical results clearly and professionally has become as important as the analysis itself. By investing in proper presentation tools and techniques, researchers can maximize the impact of their work and ensure that valuable insights reach and influence their intended audiences.\nRemember: Great statistics poorly communicated are far less valuable than good statistics clearly presented. The goal is not just to analyze data, but to transform that analysis into actionable knowledge."
  },
  {
    "objectID": "posts/datah/medical_datactor/deepseek/index.html",
    "href": "posts/datah/medical_datactor/deepseek/index.html",
    "title": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment",
    "section": "",
    "text": "Recently, a Chinese AI model called DeepSeek seemed to come out of nowhere, rocketing to the top of app charts and sending shockwaves through global financial markets. Tech enthusiasts praised its capabilities, investors scrambled to reassess their AI portfolios, and billions of dollars in market value evaporated as the West questioned its AI dominance.\nBut what if much of that excitement was manufactured?\nRecent disinformation research reveals a disturbing truth: DeepSeek’s meteoric rise was largely orchestrated by thousands of coordinated fake accounts, operating with the precision of a state-sponsored campaign. This isn’t just another case of social media manipulation—it’s a wake-up call for how easily artificial hype can trigger real financial consequences."
  },
  {
    "objectID": "posts/datah/medical_datactor/deepseek/index.html#the-anatomy-of-artificial-hype",
    "href": "posts/datah/medical_datactor/deepseek/index.html#the-anatomy-of-artificial-hype",
    "title": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment",
    "section": "The Anatomy of Artificial Hype",
    "text": "The Anatomy of Artificial Hype\nA comprehensive analysis of 41,864 profiles discussing DeepSeek uncovered a sophisticated disinformation operation:\n\n3,388 fake accounts were identified—representing 15% of all engagement on X, double the typical baseline\nThese accounts generated 2,158 posts in a single day at peak activity\n44.7% of fake profiles were created in 2024, coinciding suspiciously with DeepSeek’s launch timing\n\nThe fake accounts didn’t operate in isolation. They employed a two-pronged strategy that maximized their impact:\n\nStrategy 1: Mutual Amplification\nFake profiles systematically liked and commented on each other’s posts, creating an illusion of organic popularity. This coordinated behavior pushed DeepSeek content higher in algorithmic feeds, making it appear more engaging than it actually was.\n\n\nStrategy 2: Hijacking Authentic Conversations\nPerhaps more insidiously, bot accounts inserted themselves into genuine user discussions. By blending with real conversations, they gained credibility and influenced authentic users to engage with the manufactured narrative."
  },
  {
    "objectID": "posts/datah/medical_datactor/deepseek/index.html#the-telltale-signs-of-coordination",
    "href": "posts/datah/medical_datactor/deepseek/index.html#the-telltale-signs-of-coordination",
    "title": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment",
    "section": "The Telltale Signs of Coordination",
    "text": "The Telltale Signs of Coordination\nThe fake accounts displayed classic hallmarks of bot networks:\n\nAvatar recycling: Many profiles used generic stock photos, particularly of Chinese women\nCopy-paste content: Identical praise-filled comments appeared across multiple accounts\nSynchronized timing: Coordinated bursts of activity created artificial viral moments\nRecent creation dates: The timing aligned perfectly with DeepSeek’s market entry\n\nThese patterns match known behaviors of Chinese state-linked bot networks, suggesting this wasn’t a grassroots enthusiasm but a calculated influence operation."
  },
  {
    "objectID": "posts/datah/medical_datactor/deepseek/index.html#real-consequences-of-fake-hype",
    "href": "posts/datah/medical_datactor/deepseek/index.html#real-consequences-of-fake-hype",
    "title": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment",
    "section": "Real Consequences of Fake Hype",
    "text": "Real Consequences of Fake Hype\nThe manufactured excitement around DeepSeek had tangible impacts:\n\nMarket volatility: US tech stocks experienced significant swings as investors reacted to the perceived AI breakthrough\nBillions in market cap: Companies saw valuations fluctuate based on artificial sentiment\nStrategic misjudgments: The hype influenced narratives about the global AI arms race, potentially affecting corporate and policy decisions\n\nThis represents a new frontier in disinformation—moving beyond political influence to directly manipulating financial markets and technology adoption cycles."
  },
  {
    "objectID": "posts/datah/medical_datactor/deepseek/index.html#the-detection-challenge-build-or-buy",
    "href": "posts/datah/medical_datactor/deepseek/index.html#the-detection-challenge-build-or-buy",
    "title": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment",
    "section": "The Detection Challenge: Build or Buy?",
    "text": "The Detection Challenge: Build or Buy?\nAs these tactics become more sophisticated, organizations face a critical question: Should they develop internal detection capabilities or rely on specialized tools?\nThe case for building internally: - Full control over detection criteria - Customization for specific threats - No dependency on external vendors\nThe reality of building: - Requires extensive data pipelines across multiple platforms - Demands specialized AI expertise that’s scarce and expensive - Needs 24/7 monitoring capabilities - Takes months to develop and deploy effectively\nThe case for specialized tools: - Pre-trained to identify fake accounts and coordinated behavior - Broader platform coverage and faster deployment - Immediate insights rather than months of development - Cost-effective for most organizations\nGiven the speed at which disinformation campaigns operate—DeepSeek’s peak activity lasted just one day—the time advantage of specialized tools often outweighs the control benefits of internal development."
  },
  {
    "objectID": "posts/datah/medical_datactor/deepseek/index.html#a-90-day-response-framework",
    "href": "posts/datah/medical_datactor/deepseek/index.html#a-90-day-response-framework",
    "title": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment",
    "section": "A 90-Day Response Framework",
    "text": "A 90-Day Response Framework\nOrganizations serious about protecting themselves from manufactured hype can implement a structured approach:\nDays 1-30: Foundation - Connect monitoring dashboards to major social platforms - Establish baseline metrics for normal vs. suspicious activity - Set up alert thresholds for unusual engagement spikes\nDays 31-60: Testing - Run simulations of potential bot-driven campaigns - Align communications and risk management teams - Test response procedures under controlled conditions\nDays 61-90: Operationalization - Develop playbooks for different scenario types - Train teams on investor messaging during disinformation events - Establish clear escalation procedures for market-moving events"
  },
  {
    "objectID": "posts/datah/medical_datactor/deepseek/index.html#the-broader-implications",
    "href": "posts/datah/medical_datactor/deepseek/index.html#the-broader-implications",
    "title": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment",
    "section": "The Broader Implications",
    "text": "The Broader Implications\nThe DeepSeek case isn’t an isolated incident—it’s a preview of what’s to come. As AI competition intensifies and markets become more reactive to technological developments, the incentives for manufactured hype will only grow.\nKey questions for leaders:\n\nHow do you distinguish genuine market enthusiasm from artificial amplification?\nWhat safeguards protect your strategic decisions from manipulated narratives?\nHow quickly can your organization identify and respond to coordinated disinformation?"
  },
  {
    "objectID": "posts/datah/medical_datactor/deepseek/index.html#whats-next",
    "href": "posts/datah/medical_datactor/deepseek/index.html#whats-next",
    "title": "The DeepSeek deception: How fake accounts fooled markets and what it means for AI investment",
    "section": "What’s Next",
    "text": "What’s Next\nThe DeepSeek case shows how easily manufactured hype can influence real markets and strategic decisions. As competition in AI intensifies, these tactics will likely become more common and sophisticated.\nOrganizations need to develop better defenses against information manipulation, whether through internal capabilities or specialized tools. The cost of being fooled by the next coordinated campaign could be measured in billions.\nWhen the next AI breakthrough dominates headlines overnight, the smart money will be asking: genuine innovation or coordinated theater?"
  }
]